{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import torch\n",
    "import time\n",
    "import copy\n",
    "import pickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import os\n",
    "import sys\n",
    "import torch_geometric\n",
    "import itertools\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATConv, GCNConv, GATv2Conv\n",
    "from my_gat import my_GATConv\n",
    "from my_mlp_gat_edges import my_MLP_GATConv_edges\n",
    "from my_mlp_gat import my_MLP_GATConv\n",
    "sys.path.insert(0, os.path.abspath('../../'))\n",
    "from torch_geometric.datasets import Planetoid, Amazon\n",
    "from torch_sparse import SparseTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [Amazon(root='data/Amazon_Computers/', name='Computers'), Amazon(root='data/Amazon_Photo/', name='Photo'), Planetoid(root='data/Cora/', name='Cora'),Planetoid(root='data/PubMed/', name='PubMed'), Planetoid(root='data/CiteSeer/', name='CiteSeer')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_GAT(torch.nn.Module):\n",
    "    def __init__(self, d ,out_d, K):\n",
    "        super(Model_GAT, self).__init__()\n",
    "        \n",
    "        self.conv1 = my_GATConv(d, out_d, heads=K, bias_lin=True)\n",
    "\n",
    "    def forward(self, data):\n",
    "        \n",
    "        # 1. Obtain node embeddings \n",
    "        x = data.x\n",
    "        x = self.conv1(x, data.edge_index)\n",
    "        \n",
    "        return x.squeeze(-1)\n",
    "    \n",
    "class Model_MLP_GAT(torch.nn.Module):\n",
    "    def __init__(self, d ,out_d):\n",
    "        super(Model_MLP_GAT, self).__init__()\n",
    "        \n",
    "        self.conv1 = my_MLP_GATConv(d, out_d, 2, 16, bias=True, add_self_loops=True)\n",
    "\n",
    "    def forward(self, data):\n",
    "        \n",
    "        # 1. Obtain node embeddings \n",
    "        x = data.x\n",
    "        x, gamma = self.conv1(x, data.edge_index)\n",
    "        \n",
    "        return x.squeeze(-1), gamma.squeeze(-1)\n",
    "    \n",
    "class Model_MLP_GAT_edges(torch.nn.Module):\n",
    "    def __init__(self, d_n ,out_d_n, d_e, out_d_e):\n",
    "        super(Model_MLP_GAT_edges, self).__init__()\n",
    "        \n",
    "        self.conv1 = my_MLP_GATConv_edges(d_n, out_d_n, d_e, out_d_e, 2, 16, bias=True, add_self_loops=True)\n",
    "\n",
    "    def forward(self, data):\n",
    "        \n",
    "        # 1. Obtain node embeddings \n",
    "        x = data.x\n",
    "        e = data.e\n",
    "        x, gamma = self.conv1(x, data.edge_index, e)\n",
    "        \n",
    "        return x.squeeze(-1), gamma.squeeze(-1)\n",
    "    \n",
    "class Model_GCN(torch.nn.Module):\n",
    "    def __init__(self, d ,out_d):\n",
    "        super(Model_GCN, self).__init__()\n",
    "        \n",
    "        self.conv1 = GCNConv(d,out_d, bias=True)\n",
    "\n",
    "    def forward(self, data):\n",
    "        \n",
    "        # 1. Obtain node embeddings \n",
    "        x = data.x\n",
    "        x = self.conv1(x, data.edge_index)\n",
    "        \n",
    "        return x.squeeze(-1)\n",
    "    \n",
    "class Model_linear(torch.nn.Module):\n",
    "    def __init__(self, d ,out_d):\n",
    "        super(Model_linear, self).__init__()\n",
    "        \n",
    "        self.linear = torch.nn.Linear(d,out_d, bias=True)\n",
    "\n",
    "    def forward(self, data):\n",
    "        \n",
    "        # 1. Obtain node embeddings \n",
    "        x = data.x\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        return x.squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup train and accuracy functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, criterion, opt):\n",
    "    model.train()\n",
    "    opt.zero_grad()\n",
    "    logits = model(data) # does a forward computation\n",
    "    loss = criterion(logits[data.train_mask], data.ynew[data.train_mask]) \n",
    "    loss.backward() # this computes the stochastic gradient (or whatever gradient you are using based on the solver)\n",
    "    opt.step() # this updates the parameters using the gradient that has been computed above.\n",
    "    return loss\n",
    "        \n",
    "@torch.no_grad()\n",
    "def measure_accuracy(model, data):\n",
    "    model.eval()\n",
    "         \n",
    "    logits = model(data) # forward operation\n",
    "    preds = torch.sigmoid(logits) > 0.5\n",
    "    \n",
    "    # calculate training accuracy\n",
    "    correct = preds[data.train_mask] == data.ynew[data.train_mask]\n",
    "    train_acc = int(correct.sum()) / int(data.train_mask.sum())\n",
    "    \n",
    "    # calculate training accuracy\n",
    "    correct = preds[data.test_mask] == data.ynew[data.test_mask]\n",
    "    test_acc = int(correct.sum()) / int(data.test_mask.sum())\n",
    "        \n",
    "    return train_acc, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup GCN function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gcn(data, d, out_d, device, weight_decay, loss_tol, epochs):\n",
    "    # Define the model\n",
    "    model = Model_GCN(d, out_d=out_d).to(device)\n",
    "\n",
    "    # Define the criterion\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Define the solver, check documentation in pytorch for how to set the learning rate.\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1.0e-3, weight_decay=weight_decay)\n",
    "\n",
    "    # Test using the randomly initialized parameters.\n",
    "    train_acc, test_acc = measure_accuracy(model, data)\n",
    "\n",
    "    for epoch in range(1, epochs):\n",
    "        loss = train(model, data, criterion, opt) # Performs an Adam step etc.\n",
    "        train_acc, test_acc = measure_accuracy(model, data) # Test at each epoch\n",
    "#         if loss <= 1.0e-2 or train_acc > 0.99:\n",
    "        if loss <= loss_tol:\n",
    "            break\n",
    "#         print(f\"q: {epoch:0.1f} | Loss: {loss:0.15f} | Train: {train_acc:0.4f} | Test: {test_acc:0.4f}\")\n",
    "    return loss, train_acc, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_linear(data, d, out_d, device, weight_decay, loss_tol, epochs):\n",
    "    # Define the model\n",
    "    model = Model_linear(d, out_d=out_d).to(device)\n",
    "\n",
    "    # Define the criterion\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Define the solver, check documentation in pytorch for how to set the learning rate.\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1.0e-3, weight_decay=weight_decay)\n",
    "\n",
    "    # Test using the randomly initialized parameters.\n",
    "    train_acc, test_acc = measure_accuracy(model, data)\n",
    "\n",
    "    for epoch in range(1, epochs):\n",
    "        loss = train(model, data, criterion, opt) # Performs an Adam step etc.\n",
    "        train_acc, test_acc = measure_accuracy(model, data) # Test at each epoch\n",
    "#         if loss <= 1.0e-2 or train_acc > 0.99:\n",
    "        if loss <= loss_tol:\n",
    "            break\n",
    "#         print(f\"q: {epoch:0.1f} | Loss: {loss:0.15f} | Train: {train_acc:0.4f} | Test: {test_acc:0.4f}\")\n",
    "    return loss, train_acc, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup MLP GAT and GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gat(model, data, criterion, opt):\n",
    "    model.train()\n",
    "    opt.zero_grad()\n",
    "    logits = model(data) # does a forward computation\n",
    "    loss = criterion(logits[data.train_mask], data.ynew[data.train_mask]) \n",
    "    loss.backward() # this computes the stochastic gradient (or whatever gradient you are using based on the solver)\n",
    "    opt.step() # this updates the parameters using the gradient that has been computed above.\n",
    "    return loss\n",
    "        \n",
    "@torch.no_grad()\n",
    "def measure_accuracy_gat(model, data):\n",
    "    model.eval()\n",
    "         \n",
    "    logits = model(data) # forward operation\n",
    "    preds = torch.sigmoid(logits) > 0.5\n",
    "    \n",
    "    # calculate training accuracy\n",
    "    correct = preds[data.train_mask] == data.ynew[data.train_mask]\n",
    "    train_acc = int(correct.sum()) / int(data.train_mask.sum())\n",
    "    \n",
    "    # calculate training accuracy\n",
    "    correct = preds[data.test_mask] == data.ynew[data.test_mask]\n",
    "    test_acc = int(correct.sum()) / int(data.test_mask.sum())\n",
    "        \n",
    "    return train_acc, test_acc\n",
    "\n",
    "def train_mlp_gat(model, data, criterion, opt):\n",
    "    model.train()\n",
    "    opt.zero_grad()\n",
    "    logits, gamma = model(data) # does a forward computation\n",
    "    loss = criterion(logits[data.train_mask], data.ynew[data.train_mask]) \n",
    "    loss.backward() # this computes the stochastic gradient (or whatever gradient you are using based on the solver)\n",
    "    opt.step() # this updates the parameters using the gradient that has been computed above.\n",
    "    return loss\n",
    "        \n",
    "@torch.no_grad()\n",
    "def measure_accuracy_mlp_gat(model, data):\n",
    "    model.eval()\n",
    "         \n",
    "    logits, gamma = model(data) # forward operation\n",
    "    preds = torch.sigmoid(logits) > 0.5\n",
    "    \n",
    "    # calculate training accuracy\n",
    "    correct = preds[data.train_mask] == data.ynew[data.train_mask]\n",
    "    train_acc = int(correct.sum()) / int(data.train_mask.sum())\n",
    "    \n",
    "    # calculate training accuracy\n",
    "    correct = preds[data.test_mask] == data.ynew[data.test_mask]\n",
    "    test_acc = int(correct.sum()) / int(data.test_mask.sum())\n",
    "        \n",
    "    return train_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mlp_gat_edges(data, d_n, out_d_n, d_e, out_d_e, device, weight_decay, loss_tol, epochs):\n",
    "    # Define the model\n",
    "    model = Model_MLP_GAT_edges(d_n=d_n, out_d_n=out_d_n, d_e=d_e, out_d_e=out_d_e).to(device)\n",
    "\n",
    "    # Define the criterion\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Define the solver, check documentation in pytorch for how to set the learning rate.\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1.0e-3, weight_decay=weight_decay)\n",
    "#     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, 'min')\n",
    "\n",
    "    # Test using the randomly initialized parameters.\n",
    "    train_acc, test_acc = measure_accuracy_mlp_gat(model, data)\n",
    "\n",
    "    for epoch in range(1, epochs):\n",
    "        loss = train_mlp_gat(model, data, criterion, opt) # Performs an Adam step etc.\n",
    "        train_acc, test_acc = measure_accuracy_mlp_gat(model, data) # Test at each epoch\n",
    "#         if loss <= 1.0e-2 or train_acc > 0.99:\n",
    "        if loss <= loss_tol:\n",
    "            break\n",
    "#         print(f\"q: {epoch:0.1f} | Loss: {loss:0.15f} | Train: {train_acc:0.4f} | Test: {test_acc:0.4f}\")\n",
    "    return loss, train_acc, test_acc, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mlp_gat(data, d, out_d, device, weight_decay, loss_tol, epochs):\n",
    "    # Define the model\n",
    "    model = Model_MLP_GAT(d, out_d=out_d).to(device)\n",
    "\n",
    "    # Define the criterion\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Define the solver, check documentation in pytorch for how to set the learning rate.\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1.0e-3, weight_decay=weight_decay)\n",
    "#     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, 'min')\n",
    "\n",
    "    # Test using the randomly initialized parameters.\n",
    "    train_acc, test_acc = measure_accuracy_mlp_gat(model, data)\n",
    "\n",
    "    for epoch in range(1, epochs):\n",
    "        loss = train_mlp_gat(model, data, criterion, opt) # Performs an Adam step etc.\n",
    "        train_acc, test_acc = measure_accuracy_mlp_gat(model, data) # Test at each epoch\n",
    "#         if loss <= 1.0e-2 or train_acc > 0.99:\n",
    "        if loss <= loss_tol:\n",
    "            break\n",
    "#         print(f\"q: {epoch:0.1f} | Loss: {loss:0.15f} | Train: {train_acc:0.4f} | Test: {test_acc:0.4f}\")\n",
    "    return loss, train_acc, test_acc, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gat(data, d, out_d, K, device, weight_decay, loss_tol, epochs):\n",
    "    # Define the model\n",
    "    model = Model_GAT(d, out_d=out_d, K=K).to(device)\n",
    "\n",
    "    # Define the criterion\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Define the solver, check documentation in pytorch for how to set the learning rate.\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1.0e-3, weight_decay=weight_decay)\n",
    "#     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, 'min')\n",
    "\n",
    "    # Test using the randomly initialized parameters.\n",
    "    train_acc, test_acc = measure_accuracy_gat(model, data)\n",
    "\n",
    "    for epoch in range(1, epochs):\n",
    "        loss = train_gat(model, data, criterion, opt) # Performs an Adam step etc.\n",
    "        train_acc, test_acc = measure_accuracy_gat(model, data) # Test at each epoch\n",
    "#         if loss <= 1.0e-2 or train_acc > 0.99:\n",
    "        if loss <= loss_tol:\n",
    "            break\n",
    "#         print(f\"q: {epoch:0.1f} | Loss: {loss:0.15f} | Train: {train_acc:0.4f} | Test: {test_acc:0.4f}\")\n",
    "    return loss, train_acc, test_acc, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\multirow{20}{*}{\\rotatebox[origin=c]{90}{computers}} & \\multirow{2}{*}{$0$} & GC & $98.7$ & $1.3$ & $96.8$ \\\\\n",
      "& & GA & $98.1$ & $1.9$ & $96.7$ \\\\ \\cline{2-6}\n",
      "& \\multirow{2}{*}{$1$} & GC & $93.6$ & $6.4$ & $91.4$ \\\\\n",
      "& & GA & $93.3$ & $6.7$ & $88.7$ \\\\ \\cline{2-6}\n",
      "& \\multirow{2}{*}{$2$} & GC & $98.1$ & $1.9$ & $95.8$ \\\\\n",
      "& & GA & $97.8$ & $2.2$ & $92.1$ \\\\ \\cline{2-6}\n",
      "& \\multirow{2}{*}{$3$} & GC & $97.5$ & $2.5$ & $96.0$ \\\\\n",
      "& & GA & $96.0$ & $4.0$ & $96.0$ \\\\ \\cline{2-6}\n",
      "& \\multirow{2}{*}{$4$} & GC & $89.4$ & $10.6$ & $89.7$ \\\\\n",
      "& & GA & $89.4$ & $10.6$ & $83.3$ \\\\ \\cline{2-6}\n",
      "& \\multirow{2}{*}{$5$} & GC & $99.6$ & $0.4$ & $97.8$ \\\\\n",
      "& & GA & $99.5$ & $0.5$ & $98.0$ \\\\ \\cline{2-6}\n",
      "& \\multirow{2}{*}{$6$} & GC & $97.0$ & $3.0$ & $96.4$ \\\\\n",
      "& & GA & $96.5$ & $3.5$ & $96.5$ \\\\ \\cline{2-6}\n",
      "& \\multirow{2}{*}{$7$} & GC & $99.1$ & $0.9$ & $96.8$ \\\\\n",
      "& & GA & $98.5$ & $1.5$ & $94.9$ \\\\ \\cline{2-6}\n",
      "& \\multirow{2}{*}{$8$} & GC & $91.8$ & $8.2$ & $88.9$ \\\\\n",
      "& & GA & $91.7$ & $8.3$ & $86.5$ \\\\ \\cline{2-6}\n",
      "& \\multirow{2}{*}{$9$} & GC & $99.3$ & $0.7$ & $97.9$ \\\\\n",
      "& & GA & $99.0$ & $1.0$ & $98.2$ \\\\\n",
      "\\hline \\hline\n",
      "\\multirow{16}{*}{\\rotatebox[origin=c]{90}{photo}} & \\multirow{2}{*}{$0$} & GC & $99.0$ & $1.0$ & $95.1$ \\\\\n",
      "& & GA & $98.8$ & $1.2$ & $96.0$ \\\\ \\cline{2-6}\n",
      "& \\multirow{2}{*}{$1$} & GC & $94.9$ & $5.1$ & $94.2$ \\\\\n",
      "& & GA & $95.5$ & $4.5$ & $89.0$ \\\\ \\cline{2-6}\n",
      "& \\multirow{2}{*}{$2$} & GC & $99.4$ & $0.6$ & $96.5$ \\\\\n",
      "& & GA & $99.1$ & $0.9$ & $92.8$ \\\\ \\cline{2-6}\n",
      "& \\multirow{2}{*}{$3$} & GC & $93.6$ & $6.4$ & $91.5$ \\\\\n",
      "& & GA & $91.5$ & $8.5$ & $88.6$ \\\\ \\cline{2-6}\n",
      "& \\multirow{2}{*}{$4$} & GC & $96.2$ & $3.8$ & $88.4$ \\\\\n",
      "& & GA & $94.5$ & $5.5$ & $88.6$ \\\\ \\cline{2-6}\n",
      "& \\multirow{2}{*}{$5$} & GC & $99.7$ & $0.3$ & $98.6$ \\\\\n",
      "& & GA & $99.6$ & $0.4$ & $95.1$ \\\\ \\cline{2-6}\n",
      "& \\multirow{2}{*}{$6$} & GC & $94.3$ & $5.7$ & $86.8$ \\\\\n",
      "& & GA & $92.7$ & $7.3$ & $82.4$ \\\\ \\cline{2-6}\n",
      "& \\multirow{2}{*}{$7$} & GC & $95.7$ & $4.3$ & $95.5$ \\\\\n",
      "& & GA & $94.0$ & $6.0$ & $95.5$ \\\\\n",
      "\\hline \\hline\n",
      "\\multirow{14}{*}{\\rotatebox[origin=c]{90}{Cora}} & \\multirow{2}{*}{$0$} & GC & $94.2$ & $5.8$ & $89.5$ \\\\\n",
      "& & GA & $94.7$ & $5.3$ & $88.6$ \\\\ \\cline{2-6}\n",
      "& \\multirow{2}{*}{$1$} & GC & $97.3$ & $2.7$ & $92.3$ \\\\\n",
      "& & GA & $97.4$ & $2.6$ & $93.7$ \\\\ \\cline{2-6}\n",
      "& \\multirow{2}{*}{$2$} & GC & $97.9$ & $2.1$ & $86.9$ \\\\\n",
      "& & GA & $98.1$ & $1.9$ & $90.8$ \\\\ \\cline{2-6}\n",
      "& \\multirow{2}{*}{$3$} & GC & $93.6$ & $6.4$ & $70.7$ \\\\\n",
      "& & GA & $94.1$ & $5.9$ & $73.3$ \\\\ \\cline{2-6}\n",
      "& \\multirow{2}{*}{$4$} & GC & $96.3$ & $3.7$ & $88.8$ \\\\\n",
      "& & GA & $96.6$ & $3.4$ & $88.8$ \\\\ \\cline{2-6}\n",
      "& \\multirow{2}{*}{$5$} & GC & $96.8$ & $3.2$ & $91.7$ \\\\\n",
      "& & GA & $96.7$ & $3.3$ & $91.3$ \\\\ \\cline{2-6}\n",
      "& \\multirow{2}{*}{$6$} & GC & $98.1$ & $1.9$ & $94.9$ \\\\\n",
      "& & GA & $98.2$ & $1.8$ & $95.1$ \\\\\n",
      "\\hline \\hline\n",
      "\\multirow{6}{*}{\\rotatebox[origin=c]{90}{PubMed}} & \\multirow{2}{*}{$0$} & GC & $92.2$ & $7.8$ & $82.1$ \\\\\n",
      "& & GA & $92.1$ & $7.9$ & $82.8$ \\\\ \\cline{2-6}\n",
      "& \\multirow{2}{*}{$1$} & GC & $91.4$ & $8.6$ & $58.8$ \\\\\n",
      "& & GA & $90.6$ & $9.4$ & $59.5$ \\\\ \\cline{2-6}\n",
      "& \\multirow{2}{*}{$2$} & GC & $89.4$ & $10.6$ & $60.8$ \\\\\n",
      "& & GA & $89.1$ & $10.9$ & $61.8$ \\\\\n",
      "\\hline \\hline\n",
      "\\multirow{12}{*}{\\rotatebox[origin=c]{90}{CiteSeer}} & \\multirow{2}{*}{$0$} & GC & $94.3$ & $5.7$ & $92.0$ \\\\\n",
      "& & GA & $94.4$ & $5.6$ & $91.6$ \\\\ \\cline{2-6}\n",
      "& \\multirow{2}{*}{$1$} & GC & $92.6$ & $7.4$ & $82.6$ \\\\\n",
      "& & GA & $92.8$ & $7.2$ & $82.8$ \\\\ \\cline{2-6}\n",
      "& \\multirow{2}{*}{$2$} & GC & $92.3$ & $7.7$ & $86.2$ \\\\\n",
      "& & GA & $92.7$ & $7.3$ & $85.2$ \\\\ \\cline{2-6}\n",
      "& \\multirow{2}{*}{$3$} & GC & $93.9$ & $6.1$ & $80.0$ \\\\\n",
      "& & GA & $93.8$ & $6.2$ & $78.4$ \\\\ \\cline{2-6}\n",
      "& \\multirow{2}{*}{$4$} & GC & $94.7$ & $5.3$ & $85.9$ \\\\\n",
      "& & GA & $94.9$ & $5.1$ & $84.9$ \\\\ \\cline{2-6}\n",
      "& \\multirow{2}{*}{$5$} & GC & $96.2$ & $3.8$ & $86.5$ \\\\\n",
      "& & GA & $96.1$ & $3.9$ & $85.7$ \\\\\n",
      "\\hline \\hline\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "#     print(dataset.name)\n",
    "    data = dataset[0].to(device)\n",
    "    n_classes = data.y.max() + 1\n",
    "#     print(\"Number of classes: \", n_classes)\n",
    "    n = data.y.shape[0]\n",
    "    d = data.x.shape[1]\n",
    "    d_n = int(np.floor(d/2))\n",
    "    d_e = d - d_n\n",
    "    \n",
    "    data.e = data.x[:,0:d_n]\n",
    "    data.x = data.x[:,d_n:d]\n",
    "    \n",
    "    data.edge_index2 = torch_geometric.utils.add_remaining_self_loops(data.edge_index)[0]\n",
    "    \n",
    "    if dataset.name == 'computers' or dataset.name == 'photo':\n",
    "        data.train_mask = torch.BoolTensor(np.random.binomial(1, 0.01, size=n)) # NOTE: some training_ratio should be specifiied for Amazon and Coa\n",
    "        data.test_mask = ~data.train_mask\n",
    "\n",
    "    for which_class in range(n_classes):\n",
    "        y = torch.zeros(n, dtype=torch.float64).to(device)\n",
    "        idx = data.y == which_class\n",
    "        y[idx] = 1\n",
    "        data.ynew = y\n",
    "\n",
    "        class0 = torch.where(data.ynew == 0)[0]\n",
    "        class1 = torch.where(data.ynew == 1)[0]\n",
    "        perm = torch.cat((class0, class1), 0).cpu().detach().numpy()\n",
    "#         print(\"Class: \", which_class, \"#C0: \", class0.shape[0], \"#C1: \", class1.shape[0])\n",
    "\n",
    "        sum_test_acc_gcn = 0\n",
    "        sum_test_acc_lin = 0\n",
    "        sum_test_acc_mlp_gat = 0\n",
    "        sum_test_acc_gat = 0\n",
    "        \n",
    "        intra_gamma_ = 0\n",
    "        intra_gamma_default_ = 0\n",
    "        inter_gamma_ = 0\n",
    "        inter_gamma_default_ = 0\n",
    "\n",
    "        weight_decay = 0\n",
    "        loss_tol = 1.0e-2\n",
    "        epochs = 500\n",
    "\n",
    "        trials = 5\n",
    "        for trial in range(trials):\n",
    "\n",
    "#             print(\"trial/trials: \", (trial+1)/trials)\n",
    "\n",
    "            loss, train_acc, test_acc = run_gcn(data, d=data.x.shape[1], out_d=1, device=device, weight_decay=weight_decay, loss_tol=loss_tol, epochs=epochs)\n",
    "#             print(f\"GCN,     Loss: {loss:0.16f} | Train: {train_acc:0.4f} | Test: {test_acc:0.4f}\")\n",
    "            sum_test_acc_gcn += test_acc\n",
    "\n",
    "            loss, train_acc, test_acc = run_linear(data, d=data.x.shape[1], out_d=1, device=device, weight_decay=weight_decay, loss_tol=loss_tol, epochs=epochs)\n",
    "#             print(f\"Linear,  Loss: {loss:0.16f} | Train: {train_acc:0.4f} | Test: {test_acc:0.4f}\")\n",
    "            sum_test_acc_lin += test_acc\n",
    "\n",
    "#             loss, train_acc, test_acc, model_mlp_gat = run_mlp_gat(data, d=data.x.shape[1], out_d=1, device=device, weight_decay=weight_decay, loss_tol=loss_tol, epochs=epochs)\n",
    "#             print(f\"MLP GAT, Loss: {loss:0.1f} | Train: {train_acc:0.4f} | Test: {test_acc:0.4f}\")\n",
    "# #             logits, gamma = model_mlp_gat(data) \n",
    "#             sum_test_acc_mlp_gat += test_acc\n",
    "            \n",
    "            loss, train_acc, test_acc, model_mlp_gat_edges = run_mlp_gat_edges(data, d_n=data.x.shape[1], out_d_n=1, d_e=data.e.shape[1], out_d_e=1, device=device, weight_decay=weight_decay, loss_tol=loss_tol, epochs=epochs)\n",
    "#             print(f\"GAT with edge features, Loss: {loss:0.1f} | Train: {train_acc:0.4f} | Test: {test_acc:0.4f}\")\n",
    "            logits, gamma = model_mlp_gat_edges(data) \n",
    "            sum_test_acc_mlp_gat += test_acc\n",
    "            \n",
    "#             heads = 2\n",
    "#             loss, train_acc, test_acc, model_gat = run_gat(data, d=data.x.shape[1], out_d=1, K=heads, device=device, weight_decay=weight_decay, loss_tol=loss_tol, epochs=epochs)\n",
    "#             print(f\"GAT,     Loss: {loss:0.1f} | Train: {train_acc:0.4f} | Test: {test_acc:0.4f}\")\n",
    "#             sum_test_acc_gat += test_acc\n",
    "            \n",
    "            attn_adj = SparseTensor(row=data.edge_index2[0], col=data.edge_index2[1], value=gamma).to_scipy('csr')\n",
    "#             diffs = []\n",
    "            intra_gamma = []\n",
    "            inter_gamma = []\n",
    "            intra_gamma_default = []\n",
    "            inter_gamma_default = []\n",
    "            \n",
    "            for i in range(n):\n",
    "                neighbors = attn_adj[:,i].nonzero()[0]\n",
    "                uniform = np.ones(neighbors.shape[0]) / neighbors.shape[0]\n",
    "#                 a1 = torch.FloatTensor(uniform)\n",
    "#                 a2 = torch.FloatTensor(attn_adj[neighbors,i].data)\n",
    "#                 diff = (a2*(a2.log()-a1.log())).sum()\n",
    "#                 diffs.append(diff)\n",
    "                \n",
    "                intra_neighbors = data.ynew[i] == data.ynew[neighbors]\n",
    "                inter_neighbors = data.ynew[i] != data.ynew[neighbors]\n",
    "                \n",
    "                tmp = attn_adj[neighbors,i].data[intra_neighbors.cpu().detach().numpy()]\n",
    "                tmp_l = tmp.shape[0]\n",
    "                if tmp_l != 0:\n",
    "                    intra_gamma.append(np.sum(tmp))\n",
    "                tmp = attn_adj[neighbors,i].data[inter_neighbors.cpu().detach().numpy()]\n",
    "                tmp_l = tmp.shape[0]\n",
    "                if tmp_l != 0:\n",
    "                    inter_gamma.append(np.sum(tmp))\n",
    "                \n",
    "                tmp = uniform[intra_neighbors.cpu().detach().numpy()]\n",
    "                tmp_l = tmp.shape[0]\n",
    "                if tmp_l != 0:\n",
    "                    intra_gamma_default.append(np.sum(tmp))\n",
    "                tmp = uniform[inter_neighbors.cpu().detach().numpy()]\n",
    "                tmp_l = tmp.shape[0]\n",
    "                if tmp_l != 0:\n",
    "                    inter_gamma_default.append(np.sum(tmp))\n",
    "\n",
    "#             print(\"Mean KL distance: \", np.asarray(diffs).mean())\n",
    "            \n",
    "            total_ga = np.asarray(intra_gamma).sum() + np.asarray(inter_gamma).sum()\n",
    "            total_gc = np.asarray(intra_gamma_default).sum() + np.asarray(inter_gamma_default).sum()\n",
    "            intra_gamma_ += 100*np.asarray(intra_gamma).sum()/total_ga\n",
    "            inter_gamma_ += 100*np.asarray(inter_gamma).sum()/total_ga\n",
    "            intra_gamma_default_ += 100*np.asarray(intra_gamma_default).sum()/total_gc\n",
    "            inter_gamma_default_ += 100*np.asarray(inter_gamma_default).sum()/total_gc\n",
    "#             print(\"mass intra-edges for attention: \", 100*np.asarray(intra_gamma).sum()/total_ga, \" mass intra-edges for GC: \", 100*np.asarray(intra_gamma_default).sum()/total_gc)\n",
    "#             print(\"mass inter-edges for attention: \", 100*np.asarray(inter_gamma).sum()/total_ga, \" mass inter-edges for GC: \", 100*np.asarray(inter_gamma_default).sum()/total_gc)\n",
    "            \n",
    "#         print(\"Test accuracy GCN: \", sum_test_acc_gcn/trials)\n",
    "#         print(\"Test accuracy Linear: \", sum_test_acc_lin/trials)\n",
    "#         print(\"Test accuracy GAT: \", sum_test_acc_gat/trials)\n",
    "#         print(\"Test accuracy MLP-GAT: \", sum_test_acc_mlp_gat/trials)\n",
    "    \n",
    "#         print(\"mass intra-edges for attention: \", intra_gamma_/trials, \" mass intra-edges for GC: \", intra_gamma_default_/trials)\n",
    "#         print(\"mass inter-edges for attention: \", inter_gamma_/trials, \" mass inter-edges for GC: \", inter_gamma_default_/trials)\n",
    "        \n",
    "        if which_class == 0:\n",
    "            print(\"\\multirow{\"+str(n_classes.item()*2)+\"}{*}{\\\\rotatebox[origin=c]{90}{\"+dataset.name+\"}} & \\multirow{2}{*}{$\"+str(which_class)+\"$} & GC & $\"+str(round(intra_gamma_default_/trials,1))+\"$ & $\"+str(round(inter_gamma_default_/trials,1))+\"$ & $\"+str(round(100*sum_test_acc_gcn/trials,1))+\"$ \\\\\\\\\")\n",
    "            print(\"& & GA & $\"+str(round(intra_gamma_/trials,1))+\"$ & $\"+str(round(inter_gamma_/trials,1))+\"$ & $\"+str(round(100*sum_test_acc_mlp_gat/trials,1))+\"$ \\\\\\\\ \\cline{2-6}\")\n",
    "        elif which_class < n_classes-1:\n",
    "            print(\"& \\multirow{2}{*}{$\"+str(which_class)+\"$} & GC & $\"+str(round(intra_gamma_default_/trials,1))+\"$ & $\"+str(round(inter_gamma_default_/trials,1))+\"$ & $\"+str(round(100*sum_test_acc_gcn/trials,1))+\"$ \\\\\\\\\")\n",
    "            print(\"& & GA & $\"+str(round(intra_gamma_/trials,1))+\"$ & $\"+str(round(inter_gamma_/trials,1))+\"$ & $\"+str(round(100*sum_test_acc_mlp_gat/trials,1))+\"$ \\\\\\\\ \\cline{2-6}\")\n",
    "        else:\n",
    "            print(\"& \\multirow{2}{*}{$\"+str(which_class)+\"$} & GC & $\"+str(round(intra_gamma_default_/trials,1))+\"$ & $\"+str(round(inter_gamma_default_/trials,1))+\"$ & $\"+str(round(100*sum_test_acc_gcn/trials,1))+\"$ \\\\\\\\\")\n",
    "            print(\"& & GA & $\"+str(round(intra_gamma_/trials,1))+\"$ & $\"+str(round(inter_gamma_/trials,1))+\"$ & $\"+str(round(100*sum_test_acc_mlp_gat/trials,1))+\"$ \\\\\\\\\")\n",
    "            print(\"\\hline \\hline\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
